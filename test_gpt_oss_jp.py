#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPT-OSS B20 ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
================================

Python 3.12ç’°å¢ƒã§gpt-oss-B20ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç’°å¢ƒè¦ä»¶:
- Python 3.12.10
- PyTorch 2.6.0+cu124  
- Transformers 4.55.1
- CUDA 12.4å¯¾å¿œGPU (RTX 4060 Tiæ¨å¥¨)
"""

import torch
import time
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_system_requirements():
    """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã®ç¢ºèª"""
    logger.info("ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯é–‹å§‹")
    
    # Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    import sys
    python_version = sys.version
    logger.info(f"ğŸ Python: {python_version.split()[0]}")
    
    # PyTorch & CUDA
    logger.info(f"ğŸ”¥ PyTorch: {torch.__version__}")
    logger.info(f"âš¡ CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
        logger.info(f"ğŸš€ GPU: {gpu_name}")
        logger.info(f"ğŸ’¾ GPU Memory: {gpu_memory}GB")
    else:
        logger.warning("âš ï¸ CUDA not available - CPU inference only")
    
    # ã‚·ã‚¹ãƒ†ãƒ RAM
    ram_total = psutil.virtual_memory().total // (1024**3)
    ram_available = psutil.virtual_memory().available // (1024**3)
    logger.info(f"ğŸ§  System RAM: {ram_available}GB/{ram_total}GB available")
    
    return torch.cuda.is_available()

def load_gpt_oss_model(model_name="microsoft/DialoGPT-medium", use_cuda=True):
    """
    GPT-OSSãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (ãƒ†ã‚¹ãƒˆç”¨ã«DialoGPTã‚’ä½¿ç”¨)
    å®Ÿéš›ã®gpt-oss-B20ãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã‚‰ã€model_nameã‚’å¤‰æ›´
    """
    logger.info(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {model_name}")
    
    try:
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = "cuda" if use_cuda and torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ–¥ï¸ Device: {device}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        logger.info("ğŸ“‹ Tokenizer loading...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        logger.info("ğŸ¤– Model loading...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True
        )
        
        if device == "cpu":
            model = model.to(device)
            
        logger.info("âœ… Model loaded successfully!")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() // (1024**3)
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory // (1024**3)
            logger.info(f"ğŸ’¾ GPU Memory Used: {gpu_memory_used}GB/{gpu_memory_total}GB")
        
        return tokenizer, model, device
        
    except Exception as e:
        logger.error(f"âŒ Model loading failed: {e}")
        return None, None, None

def test_text_generation(tokenizer, model, device, prompt="ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã‚“ãªæ—¥ã§ã™ã‹ï¼Ÿ"):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info(f"ğŸ“ Prompt: {prompt}")
    
    try:
        # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)
        
        # æ¨è«–å®Ÿè¡Œ
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 50,  # 50ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ ç”Ÿæˆ
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                attention_mask=torch.ones_like(inputs)
            )
        
        generation_time = time.time() - start_time
        
        # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        logger.info(f"ğŸ“„ Generated text: {generated_text}")
        logger.info(f"â±ï¸ Generation time: {generation_time:.2f}s")
        logger.info(f"ğŸ“Š Tokens/second: {50/generation_time:.2f}")
        
        return generated_text
        
    except Exception as e:
        logger.error(f"âŒ Text generation failed: {e}")
        return None

def benchmark_performance(tokenizer, model, device, num_tests=3):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    logger.info(f"ğŸ Performance benchmark ({num_tests} tests)")
    
    test_prompts = [
        "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        "ãŠã™ã™ã‚ã®æœ¬ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    ]
    
    total_times = []
    
    for i, prompt in enumerate(test_prompts[:num_tests]):
        logger.info(f"ğŸ“‹ Test {i+1}/{num_tests}: {prompt[:30]}...")
        
        start_time = time.time()
        result = test_text_generation(tokenizer, model, device, prompt)
        test_time = time.time() - start_time
        
        total_times.append(test_time)
        
        if result:
            logger.info(f"âœ… Test {i+1} completed in {test_time:.2f}s")
        else:
            logger.error(f"âŒ Test {i+1} failed")
    
    if total_times:
        avg_time = sum(total_times) / len(total_times)
        logger.info(f"ğŸ“Š Average generation time: {avg_time:.2f}s")
        logger.info(f"ğŸš€ Performance: {50/avg_time:.2f} tokens/second")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ GPT-OSS B20 ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯
    cuda_available = check_system_requirements()
    logger.info("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    tokenizer, model, device = load_gpt_oss_model(use_cuda=cuda_available)
    
    if tokenizer is None or model is None:
        logger.error("âŒ Model loading failed. Exiting.")
        return
    
    logger.info("=" * 60)
    
    # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
    test_text_generation(tokenizer, model, device)
    logger.info("=" * 60)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    benchmark_performance(tokenizer, model, device)
    logger.info("=" * 60)
    
    logger.info("ğŸ‰ GPT-OSS B20 ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info("ğŸ§¹ GPU memory cleared")

if __name__ == "__main__":
    main()
